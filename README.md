# Data Engineering Portfolio

I'm currently a data engineer at MuzeData where I leverage a AWS-based tech stack along with Python, AirFlow & SQL to build data pipelines that yield business insights. I'm excited to share my still developing data engineer portfolio. Within this repository, you'll find a comprehensive catalog of projects completed in various data analytics/engineering courses each of which covers essential skills and techniques.

### [ETL Pipeline - AWS Data Services](https://github.com/ChisomOrika/Youtube-ETL-Analysis) 

- Brief Overview: This project aims to securely manage, streamline, and perform analysis on the structured and semi-structured YouTube videos data based on the video categories and the trending metrics.
- Tools - AWS Lambda, AWS Glue, AWSS3, Athena, Quicksight, Python

### [Dbt - Logistics ETL and analysis](https://github.com/ChisomOrika/ChambauI-Inc-Analytics)

- Brief Overview This project is focused on utilizing dbt, a data transformation tool and Python to build a data pipeline. The datasets contain information about orders, deliveries and reviews. The pipeline will transform this raw data into a format that is suitable for analysis and reporting in downstream applications.
- Technology used: AWS S3, Python, DBT, PowerBI

### [ETL Pipeline and Model - Fraud Detection](https://github.com/ChisomOrika/Fraud-prediction-pipeline)
- Brief Overview: A Digital Wallet company has quite a large amount of online transaction data. The company wants to use online transaction data to detect online payment fraud that harms their business. Create a data pipeline that can be utilised for analysis and reporting to determine whether online transaction data has excellent data quality and can be used to detect fraud in online transactions.
- Technology Used: Spark, dbt, Kafka, Docker, BigQuery Looker


### [Apache Kafka/Spark - Real time streaming pipeline with Spark and Kafka](https://github.com/ChisomOrika/User-Profile-Pipeline)

- Brief Overview; Fetch data  using Airflow DAG pipelines and store the data in Postgres DB. The entire streaming process is managed by a Kafka setup that has a Zookeeper pipeline to manage multiple broadcasts and process them from the message queue. There is a master-worker architecture setup on Apache Spark. Finally there is a Cassandra DB setup that has a listener that takes the stream data from Spark and stores in a columnar format. The entire project is containerized with Docker.
- Tools used: Airflow, Kafka, Spark.


